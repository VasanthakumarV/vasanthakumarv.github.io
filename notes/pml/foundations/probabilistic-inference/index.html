<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Vasanth</title>

    <meta charset="utf-8">
    <meta name='viewport' content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" 
      href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
      integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
      crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
      integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
      crossorigin="anonymous">
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
      integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);">
    </script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
              ]
          });
      });
    </script>
    <script>
      function snackbar() {
        var x = document.getElementById("topnav");
        if (x.className === "topnav") {
          x.className += " responsive";
        } else {
          x.className = "topnav";
        }
      }

      window.addEventListener("load", setupAccordian);
      function setupAccordian() {
        document.querySelectorAll(".accordion").forEach(function(acc, i) {
          if (i == 0) {
            acc.nextElementSibling.style.display = "block";
            acc.classList.toggle("active");
          }

          acc.addEventListener("click", function() {
            this.classList.toggle("active");
            var panel = this.nextElementSibling;
            if (panel.style.display === "block") {
              panel.style.display = "none";
            } else {
              panel.style.display = "block";
            }
          });
        });
      }
    </script>

    <link rel="stylesheet" href="/site.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  </head>

  <body>
    <nav id="topnav" class="topnav">
      <a href="/"
        >
        Home
      </a>
      <a href="/notes"
         class="navbar-selected" >
        Notes
      </a>
      <a href="/algorithms"
        >
        Algorithms
      </a>
      <a href="/projects"
        >
        Projects
      </a>
      <a href="javascript:void(0);" class="icon" onclick="snackbar()">
        â˜°
      </a>
    </nav>
    <section class="section">
      <div class="container">
        
  <h1 class="title">
    Probabilistic inference
  </h1>

  <div class="flex-container">
    <div class="flex-toc">
      <h1>Contents</h1>
      
        <ul>
        
          <li>
            <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#bayesian-inference">Bayesian Inference</a>
            
          </li>
        
          <li>
            <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#bayesian-concept-learning">Bayesian Concept Learning</a>
            
              <ul>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#likelihood">Likelihood</a>
                  </li>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#prior">Prior</a>
                  </li>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#posterior">Posterior</a>
                  </li>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#maximum-a-posterior-map">Maximum a Posterior (MAP)</a>
                  </li>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#maximum-likelihood-estimate-mle">Maximum Likelihood Estimate (MLE)</a>
                  </li>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#learning-a-continuous-concept">Learning a continuous concept</a>
                  </li>
                
              </ul>
            
          </li>
        
          <li>
            <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#bayesian-machine-learning">Bayesian Machine Learning</a>
            
              <ul>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/probabilistic-inference/#plugin-approximation">Plugin Approximation</a>
                  </li>
                
              </ul>
            
          </li>
        
        </ul>
      
    </div>

    <div class="flex-body">
      <h1 id="bayesian-inference">Bayesian Inference</h1>
<p><strong>Inference</strong> is the process of arriving at generalizations using sample data. <strong>Bayesian Inference</strong> uses <em>Bayes' Theorem</em> to model the probability of different hypotheses.</p>
<p>$$p(H=h|Y=y) = \frac{p(H=h)p(Y=y|H=h)}{p(Y=y)}$$</p>
<ul>
<li>$p(H=h|Y=y)$ is the <em>posterior distribution</em></li>
<li>$p(H=h)$ is the <em>prior distribution</em></li>
<li>$p(Y=y|H=h)$ is the <em>likelihood</em>, this is not a probability distribution</li>
<li>$p(Y=y)$ is the <em>marginal likelihood</em></li>
</ul>
<h1 id="bayesian-concept-learning">Bayesian Concept Learning</h1>
<p>We can model the distribution of the hidden quantity of interest $h \in H$ from a set of observations, $D = {y_n : n = 1 : N}$, the goal is to infer the <em>hidden patterns</em> or <em>concept</em> from the underlying data.</p>
<p>Humans can <em>generalize</em> a sequence of numbers, based on whether they are all powers of 2, or whether they are even numbers and other similar hypotheses. How do we emulate such process in machines. One classic approach is <strong>Induction</strong>, here we move from a <em>hypotheses space</em> $H$ of concepts to <em>version space</em> based on the observed data $D$, as more data comes in, we become more certain and the version space shrinks. But the version space theory cannot explain why people choose  <em>powers of two</em> when they see $D = \{ 16, 8, 2, 64 \}$ and not <em>all even numbers</em> or <em>powers of 2 except 32</em>. Bayesian Inference can help explain this behavior.</p>
<h2 id="likelihood">Likelihood</h2>
<p>To explain why people go with $h_{two}$ and not $h_{even}$ we will have to understand that it will <strong>suspiciously coincidental</strong> for all the examples in $D = \{16, 8, 2, 64\}$ to be powers of two if the true underlying hypothesis was $h_{even}$.</p>
<h2 id="prior">Prior</h2>
<p>Priori is usefull because, the hypothesis <em>powers of 2 except 32</em> is more likely than <em>powers of 2</em>, but the former more likely hypothesis is <em>unnatural</em> and we can eliminate such concepts by assigning low priors. Background knowledge or Domain knowledge can be brought into a problem using the prior values.</p>
<h2 id="posterior">Posterior</h2>
<p>Posterior is proportional to likelihood times prior, find below an image that shows how both influence the posterior in identifying the most likely hypothesis for the observed $D$.</p>
<div class="image-center">
<img src="https:&#x2F;&#x2F;vasanthakumarv.github.io&#x2F;processed_images&#x2F;cf978ce29554cfd700.png" />
</div>
<h2 id="maximum-a-posterior-map">Maximum a Posterior (MAP)</h2>
<p><strong>MAP</strong> is defined as the hypothesis with the maximum posterior probability.</p>
<p>$$h_{map} \triangleq \argmax_h(h|D)$$</p>
<h2 id="maximum-likelihood-estimate-mle">Maximum Likelihood Estimate (MLE)</h2>
<p>In MAP as the data size increases the prior term stays constant, hence it is reasonable to approximate MAP and just pick maximum likelihood estimate or <strong>MLE</strong></p>
<p>$$h_{mle} \triangleq \argmax_{h}p(D|h) = \argmax_{h}\log p(D|h) = \argmax_{h}\sum_{n=1}^{N}\log p(y_n|h)$$</p>
<h2 id="learning-a-continuous-concept">Learning a continuous concept</h2>
<p>The game is called <strong>healthy levels</strong>, we are tasked with identifying the healthy range for two continuous variables <em>cholestrol</em> and <em>insulin</em> from healthy patients, we will be provided with only positive examples. The hypotheses space will be axis parallel rectangles, it can be represented as $h = (l_1, l_2, s_1, s_2)$, where $l_j \in [-\infty, \infty]$ and represents the lower left corner of the rectangle, $s_j \in [0, \infty]$ are the lengths of the two sides.</p>
<h3 id="likelihood-1">Likelihood</h3>
<p>For 1d we will have $p(D|l,s) = (\frac{1}{s})^N$ if all the points are inside the interval. For 2d we assume the features are conditionally independent given the hypothesis.</p>
<p>$$p(D|h) = p(D_1|l_1,s_1)p(D_2|l_2,s_2)$$</p>
<h3 id="prior-1">Prior</h3>
<p>We will use uninformative priors of the form $p(h) \propto \frac{1}{s_1}\frac{1}{s_2}$.</p>
<h3 id="posterior-1">Posterior</h3>
<p>Posterior is given by,</p>
<p>$$p(l_1, l_2, s_1, s_2|D) \propto p(D_1|l_1,s_1)p(D_2|l_2,s_2)\frac{1}{s_1}\frac{1}{s_2}$$</p>
<h1 id="bayesian-machine-learning">Bayesian Machine Learning</h1>
<p>In many applications along with each unknown output $\bold{y}$ we will have features $\bold{x} \in X$ associated with it, in such cases we want to use conditional probability distribution of the form $p(\bold{y}|\bold{x},D)$, where $D = \{(\bold{x}_n,\bold{y}_n) : n = 1:N\}$. If the output is from a low-dimensional space, such as a set of labels $y \in \{1,...,C\}$ or scalars $y \in \mathbb{R}$, then we call it <strong>discriminative model</strong>, since it discriminates between different possbile outputs. If the output is high dimensional like images or sentences, it is called a <strong>conditinal generative model</strong>. </p>
<p>In these complex settings, the hypothesis or concept $h$ predictst the output given the input and set of real valued parameters $\theta \in \mathbb{R}^K$.</p>
<p>$$p(\bold{y}|\bold{x},\theta) = p(\bold{y}|f(\bold{x};\theta))$$</p>
<p>Where $f(\bold{x};\theta)$ maps the inputs to the parameters of the output distribution. To fully specify the model, we need to choose the output probability distribution and the form of $f$.</p>
<h2 id="plugin-approximation">Plugin Approximation</h2>
<p>Here we fit a model to compute a point estimate $\hat{\theta}$ and then use it to make predictions. The posterior is represented using <strong>Dirac delta function</strong>, the point estimates of the posterior are computed using <strong>MLE</strong> or <strong>MAP</strong>. The <strong>sifting property</strong> of the dirac delta helps us arrive at the final form.</p>
<p>$$p(\bold{y}|D) = \int p(\bold{y}|\theta) p(\theta|D) d\theta \approx \int p(\bold{y}|\theta)\delta(\theta - \hat{\theta}) d\theta = p(\bold{y}|\hat{\theta})$$</p>

    </div>
  </div>

      </div>
    </section>
  </body>
</html>
