<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Vasanth</title>

    <meta charset="utf-8">
    <meta name='viewport' content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" 
      href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
      integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
      crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
      integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
      crossorigin="anonymous">
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
      integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);">
    </script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
              delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
              ]
          });
      });
    </script>
    <script>
      function snackbar() {
        var x = document.getElementById("topnav");
        if (x.className === "topnav") {
          x.className += " responsive";
        } else {
          x.className = "topnav";
        }
      }

      window.addEventListener("load", setupAccordian);
      function setupAccordian() {
        document.querySelectorAll(".accordion").forEach(function(acc, i) {
          if (i == 0) {
            acc.nextElementSibling.style.display = "block";
            acc.classList.toggle("active");
          }

          acc.addEventListener("click", function() {
            this.classList.toggle("active");
            var panel = this.nextElementSibling;
            if (panel.style.display === "block") {
              panel.style.display = "none";
            } else {
              panel.style.display = "block";
            }
          });
        });
      }
    </script>

    <link rel="stylesheet" href="/site.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  </head>

  <body>
    <nav id="topnav" class="topnav">
      <a href="/"
        >
        Home
      </a>
      <a href="/notes"
         class="navbar-selected" >
        Notes
      </a>
      <a href="/algorithms"
        >
        Algorithms
      </a>
      <a href="/projects"
        >
        Projects
      </a>
      <a href="javascript:void(0);" class="icon" onclick="snackbar()">
        â˜°
      </a>
    </nav>
    <section class="section">
      <div class="container">
        
  <h1 class="title">
    Information theory
  </h1>

  <div class="flex-container">
    <div class="flex-toc">
      <h1>Contents</h1>
      
        <ul>
        
          <li>
            <a href="https://vasanthakumarv.github.io/notes/pml/foundations/information-theory/#entropy">Entropy</a>
            
              <ul>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/information-theory/#entropy-for-discrete-random-variables">Entropy for discrete random variables</a>
                  </li>
                
                  <li>
                    <a href="https://vasanthakumarv.github.io/notes/pml/foundations/information-theory/#cross-entropy">Cross entropy</a>
                  </li>
                
              </ul>
            
          </li>
        
          <li>
            <a href="https://vasanthakumarv.github.io/notes/pml/foundations/information-theory/#relative-entropy-kl-divergence">Relative entropy (KL divergence)</a>
            
          </li>
        
          <li>
            <a href="https://vasanthakumarv.github.io/notes/pml/foundations/information-theory/#mutual-information">Mutual information</a>
            
          </li>
        
        </ul>
      
    </div>

    <div class="flex-body">
      <h1 id="entropy">Entropy</h1>
<p>The <strong>entropy</strong> of a probability distribution can be intepreted as a measure of uncertainty, or lack of predictability, associated with a random variable drawn from a given distribution. We can also use entropy to define <strong>information content</strong> of a data source. A distribution wit high entropy will have high information content.</p>
<h2 id="entropy-for-discrete-random-variables">Entropy for discrete random variables</h2>
<p>The entropy of a discrete random variable $X$ with distribution $p$ over $K$ states is defined by</p>
<p>$$\mathbb{H}(X) \triangleq \sum_{k = 1}^{K} p(X = k) \text{log}_2(X = k) = -\mathbb{E}_X[\text{log\ }p(X)]$$</p>
<h2 id="cross-entropy">Cross entropy</h2>
<p>The cross entropy between distribution $p$ and $q$ is defined by</p>
<p>$$\mathbb{H}(p, q) \triangleq -\sum_{k=1}^{K} p_k \text{log\ } q_k$$</p>
<h1 id="relative-entropy-kl-divergence">Relative entropy (KL divergence)</h1>
<p>Given two distribution $p$ and $q$, it is often useful to define a distance metric to measure how &quot;close&quot; or &quot;similar&quot; they are. More precisely, we say that $D$ is a divergence if $D(p, q) \geq 0$ with equality iff $p = q$, it does not have to be a metric and satisfy triangle inequality $D(p, r) \leq D(p, q) + D(q, r)$.</p>
<p>For discrete distributions, the KL divergence is defined as follows:</p>
<p>$$\mathbb{KL}(p || q) \triangleq \sum_{k = 1}^{K} p_k \text{log\ } \frac{p_k}{q_k}$$</p>
<p>This naturally extends to continuous distributions as well:</p>
<p>$$\mathbb{KL} p(p || q) \triangleq \int p(x)\ \text{log\ } \frac{p(x)}{q(x)}\ dx$$</p>
<h1 id="mutual-information">Mutual information</h1>
<p>The KL divergence gave us a way to measure how similar two distributions were. How should we measure how dependent two random variables are? One thing we could do is turn the question of measureing the dependence of two random variables into a question about the similarity of their distribution. This gives rise to the notion of <strong>mututal information</strong> (MI) between two random variables, which is defined below.</p>
<p>$$\mathbb{I} (X; Y) \triangleq \mathbb{KL} (p(x, y) || p(x)p(y)) = \sum_{y \in Y} \sum_{x \in X} p(x, y)\ \text{log\ } \frac{p(x, y)}{p(x) p(y)}$$</p>
<p>MI measures the information gain if we update from a model that treats the two variables as independent $p(x)p(y)$ to one that models their true joint density $p(x, y)$.</p>

    </div>
  </div>

      </div>
    </section>
  </body>
</html>
